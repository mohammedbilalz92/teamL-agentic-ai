{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269743a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=15, completion_tokens=7) cached=False logprobs=None thought=None\n",
      "----------------------------------\n",
      "Hereâ€™s an interesting fact you might not know about the sky: The sky is not actually blue; it only appears blue to us during the day because of Rayleigh scattering. This scattering causes shorter blue wavelengths of sunlight to scatter in all directions more than other colors, making the sky look blue. However, if you were standing on the Moon or in space, the sky would appear completely black, even during the day, because there is no atmosphere to scatter the sunlight. \n",
      "\n",
      "Would you like to know more fascinating sky facts?\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "#direct llm call\n",
    "response = await model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(response)\n",
    "print('----------------------------------')\n",
    "\n",
    "#build agent\n",
    "chat_assistant = AssistantAgent(name=\"chat_agent\", model_client=model_client)\n",
    "result = await chat_assistant.run(task=\"tell something I did not know about the sky\")\n",
    "\n",
    "print(result.messages[-1].content)\n",
    "print('----------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e22f0837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first President of the United States was George Washington. He served as the first president from April 30, 1789, to March 4, 1797. Washington was a key figure in the American Revolutionary War as the Commander-in-Chief of the Continental Army and later played a crucial role in the framing of the U.S. Constitution. His presidency set many precedents for the new nation, including the formation of the Cabinet and the tradition of serving only two terms.\n"
     ]
    }
   ],
   "source": [
    "history_assistant = AssistantAgent(\n",
    "    name = 'history_expert',\n",
    "    model_client=model_client,\n",
    "    description='A knowledgeable assistant with expertise in world history',\n",
    "    system_message='You are a history expert with deep knowledge of world history. Provide detailed and accurate answers about historical events, figures and timelines'\n",
    ")\n",
    "\n",
    "async def test_history_expert():\n",
    "    result = await history_assistant.run(task = 'Who was the first President of USA?')\n",
    "    print(result.messages[-1].content)\n",
    "\n",
    "await test_history_expert()\n",
    "await model_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1305e",
   "metadata": {},
   "source": [
    "## Model from Open Router - It gives api key and access to many Models(including free one's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecec8c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ Using key: sk-or-v1-bdaa7794429...\n",
      "\n",
      "ðŸ”„ Checking OpenRouter...\n",
      "\n",
      "âœ… Connected successfully!\n",
      "\n",
      "ðŸ“Š Total models: 343\n",
      "ðŸ†“ Free models: 46\n",
      "ðŸ’° Paid models: 297\n",
      "\n",
      "============================================================\n",
      "ðŸ†“ FREE MODELS YOU CAN USE:\n",
      "============================================================\n",
      "\n",
      "1. nvidia/nemotron-nano-12b-v2-vl:free\n",
      "   Context: 128,000 tokens\n",
      "   Rate: 0\n",
      "\n",
      "2. minimax/minimax-m2:free\n",
      "   Context: 204,800 tokens\n",
      "   Rate: 0\n",
      "\n",
      "3. alibaba/tongyi-deepresearch-30b-a3b:free\n",
      "   Context: 131,072 tokens\n",
      "   Rate: 0\n",
      "\n",
      "4. meituan/longcat-flash-chat:free\n",
      "   Context: 131,072 tokens\n",
      "   Rate: 0\n",
      "\n",
      "5. nvidia/nemotron-nano-9b-v2:free\n",
      "   Context: 128,000 tokens\n",
      "   Rate: 0\n",
      "\n",
      "6. deepseek/deepseek-chat-v3.1:free\n",
      "   Context: 163,800 tokens\n",
      "   Rate: 0\n",
      "\n",
      "7. openai/gpt-oss-20b:free\n",
      "   Context: 131,072 tokens\n",
      "   Rate: 0\n",
      "\n",
      "8. z-ai/glm-4.5-air:free\n",
      "   Context: 131,072 tokens\n",
      "   Rate: 0\n",
      "\n",
      "9. qwen/qwen3-coder:free\n",
      "   Context: 262,000 tokens\n",
      "   Rate: 0\n",
      "\n",
      "10. moonshotai/kimi-k2:free\n",
      "   Context: 32,768 tokens\n",
      "   Rate: 0\n",
      "\n",
      "11. cognitivecomputations/dolphin-mistral-24b-venice-edition:free\n",
      "   Context: 32,768 tokens\n",
      "   Rate: 0\n",
      "\n",
      "12. google/gemma-3n-e2b-it:free\n",
      "   Context: 8,192 tokens\n",
      "   Rate: 0\n",
      "\n",
      "13. tngtech/deepseek-r1t2-chimera:free\n",
      "   Context: 163,840 tokens\n",
      "   Rate: 0\n",
      "\n",
      "14. mistralai/mistral-small-3.2-24b-instruct:free\n",
      "   Context: 131,072 tokens\n",
      "   Rate: 0\n",
      "\n",
      "15. deepseek/deepseek-r1-0528-qwen3-8b:free\n",
      "   Context: 131,072 tokens\n",
      "   Rate: 0\n",
      "\n",
      "... and 31 more free models\n",
      "\n",
      "============================================================\n",
      "ðŸ’¡ RECOMMENDED MODELS TO TRY:\n",
      "============================================================\n",
      "âœ… google/gemini-2.0-flash-exp:free\n",
      "âœ… meta-llama/llama-3.2-3b-instruct:free\n",
      "âœ… mistralai/mistral-7b-instruct:free\n",
      "âŒ qwen/qwen-2-7b-instruct:free (not available)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def check_my_models():\n",
    "    api_key = os.getenv(\"OPEN_ROUTER_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"âŒ API key not found in .env file\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ”‘ Using key: {api_key[:20]}...\")\n",
    "    print(\"\\nðŸ”„ Checking OpenRouter...\\n\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get all models\n",
    "        response = requests.get(\n",
    "            \"https://openrouter.ai/api/v1/models\",\n",
    "            headers=headers,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            models = response.json()['data']\n",
    "            \n",
    "            # Separate free and paid\n",
    "            free_models = [m for m in models if ':free' in m['id']]\n",
    "            paid_models = [m for m in models if ':free' not in m['id']]\n",
    "            \n",
    "            print(f\"âœ… Connected successfully!\\n\")\n",
    "            print(f\"ðŸ“Š Total models: {len(models)}\")\n",
    "            print(f\"ðŸ†“ Free models: {len(free_models)}\")\n",
    "            print(f\"ðŸ’° Paid models: {len(paid_models)}\\n\")\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"ðŸ†“ FREE MODELS YOU CAN USE:\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            if free_models:\n",
    "                for i, model in enumerate(free_models[:15], 1):  # Show first 15\n",
    "                    print(f\"\\n{i}. {model['id']}\")\n",
    "                    print(f\"   Context: {model.get('context_length', 'N/A'):,} tokens\")\n",
    "                    if model.get('pricing'):\n",
    "                        print(f\"   Rate: {model.get('pricing', {}).get('prompt', 'Free')}\")\n",
    "                \n",
    "                if len(free_models) > 15:\n",
    "                    print(f\"\\n... and {len(free_models) - 15} more free models\")\n",
    "            else:\n",
    "                print(\"âš ï¸ No free models found!\")\n",
    "                print(\"You may need to enable data sharing at:\")\n",
    "                print(\"https://openrouter.ai/settings/privacy\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ðŸ’¡ RECOMMENDED MODELS TO TRY:\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            recommended = [\n",
    "                \"google/gemini-2.0-flash-exp:free\",\n",
    "                \"qwen/qwen-2-7b-instruct:free\",\n",
    "                \"meta-llama/llama-3.2-3b-instruct:free\",\n",
    "                \"mistralai/mistral-7b-instruct:free\"\n",
    "            ]\n",
    "            \n",
    "            available_recommended = [m for m in free_models if m['id'] in recommended]\n",
    "            \n",
    "            for model in available_recommended:\n",
    "                print(f\"âœ… {model['id']}\")\n",
    "            \n",
    "            for rec in recommended:\n",
    "                if rec not in [m['id'] for m in available_recommended]:\n",
    "                    print(f\"âŒ {rec} (not available)\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ Error: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            \n",
    "            if response.status_code == 401:\n",
    "                print(\"\\nâš ï¸ Authentication failed!\")\n",
    "                print(\"1. Check your API key is correct\")\n",
    "                print(\"2. Make sure it starts with 'sk-or-v1-'\")\n",
    "                print(\"3. Generate a new key at: https://openrouter.ai/keys\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        print(\"\\nðŸ’¡ Try:\")\n",
    "        print(\"1. pip install requests\")\n",
    "        print(\"2. Check internet connection\")\n",
    "        print(\"3. Verify API key in .env file\")\n",
    "\n",
    "# Run it\n",
    "check_my_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc839c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing model: google/gemini-2.0-flash-exp:free\n",
      "\n",
      "âŒ FAILED: 429\n",
      "Error: {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"google/gemini-2.0-flash-exp:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Google AI Studio\"}},\"user_id\":\"user_34skxHkEh956UXtaKnaxlvgWMxr\"}\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ§ª Testing model: qwen/qwen-2-7b-instruct:free\n",
      "\n",
      "âŒ FAILED: 404\n",
      "Error: {\"error\":{\"message\":\"No endpoints found for qwen/qwen-2-7b-instruct:free.\",\"code\":404},\"user_id\":\"user_34skxHkEh956UXtaKnaxlvgWMxr\"}\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ§ª Testing model: meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "âŒ FAILED: 404\n",
      "Error: {\"error\":{\"message\":\"No allowed providers are available for the selected model.\",\"code\":404}}\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def test_model(model_name):\n",
    "    api_key = os.getenv(\"OPEN_ROUTER_API_KEY\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Say 'Hello!' in one word\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ§ª Testing model: {model_name}\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=data,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            message = result['choices'][0]['message']['content']\n",
    "            print(f\"âœ… SUCCESS! Model works!\")\n",
    "            print(f\"ðŸ“ Response: {message}\")\n",
    "        else:\n",
    "            print(f\"âŒ FAILED: {response.status_code}\")\n",
    "            print(f\"Error: {response.text}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Test these models one by one\n",
    "models_to_test = [\n",
    "    \"google/gemini-2.0-flash-exp:free\",\n",
    "    \"qwen/qwen-2-7b-instruct:free\",\n",
    "    \"meta-llama/llama-3.2-3b-instruct:free\"\n",
    "]\n",
    "\n",
    "for model in models_to_test:\n",
    "    test_model(model)\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c99f99",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     12\u001b[39m open_router_model_client = OpenAIChatCompletionClient(\n\u001b[32m     13\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://openrouter.ai/api/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgoogle/gemini-2.0-flash-exp:free\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     }\n\u001b[32m     23\u001b[39m )\n\u001b[32m     26\u001b[39m assistant_agent1 = AssistantAgent(\n\u001b[32m     27\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mhelpful_agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     model_client=open_router_model_client,\n\u001b[32m     29\u001b[39m     system_message=\u001b[33m'\u001b[39m\u001b[33mYou are a helpfull assistant agent\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m assistant_agent1.run(task=\u001b[33m'\u001b[39m\u001b[33mhey how are you today?\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py:149\u001b[39m, in \u001b[36mBaseChatAgent.run\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_messages(input_messages, cancellation_token)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.inner_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     output_messages += response.inner_messages\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:896\u001b[39m, in \u001b[36mAssistantAgent.on_messages\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_messages\u001b[39m(\n\u001b[32m    883\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    884\u001b[39m     messages: Sequence[BaseChatMessage],\n\u001b[32m    885\u001b[39m     cancellation_token: CancellationToken,\n\u001b[32m    886\u001b[39m ) -> Response:\n\u001b[32m    887\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Process incoming messages and generate a response.\u001b[39;00m\n\u001b[32m    888\u001b[39m \n\u001b[32m    889\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    894\u001b[39m \u001b[33;03m        Response containing the agent's reply\u001b[39;00m\n\u001b[32m    895\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_messages_stream(messages, cancellation_token):\n\u001b[32m    897\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[32m    898\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:953\u001b[39m, in \u001b[36mAssistantAgent.on_messages_stream\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# STEP 4: Run the first inference\u001b[39;00m\n\u001b[32m    952\u001b[39m model_result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_llm(\n\u001b[32m    954\u001b[39m     model_client=model_client,\n\u001b[32m    955\u001b[39m     model_client_stream=model_client_stream,\n\u001b[32m    956\u001b[39m     system_messages=system_messages,\n\u001b[32m    957\u001b[39m     model_context=model_context,\n\u001b[32m    958\u001b[39m     workbench=workbench,\n\u001b[32m    959\u001b[39m     handoff_tools=handoff_tools,\n\u001b[32m    960\u001b[39m     agent_name=agent_name,\n\u001b[32m    961\u001b[39m     cancellation_token=cancellation_token,\n\u001b[32m    962\u001b[39m     output_content_type=output_content_type,\n\u001b[32m    963\u001b[39m     message_id=message_id,\n\u001b[32m    964\u001b[39m ):\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[32m    966\u001b[39m         model_result = inference_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:1109\u001b[39m, in \u001b[36mAssistantAgent._call_llm\u001b[39m\u001b[34m(cls, model_client, model_client_stream, system_messages, model_context, workbench, handoff_tools, agent_name, cancellation_token, output_content_type, message_id)\u001b[39m\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1109\u001b[39m     model_result = \u001b[38;5;28;01mawait\u001b[39;00m model_client.create(\n\u001b[32m   1110\u001b[39m         llm_messages,\n\u001b[32m   1111\u001b[39m         tools=tools,\n\u001b[32m   1112\u001b[39m         cancellation_token=cancellation_token,\n\u001b[32m   1113\u001b[39m         json_output=output_content_type,\n\u001b[32m   1114\u001b[39m     )\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:704\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, tool_choice, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    703\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_params.response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     result = cast(ParsedChatCompletion[Any], result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2603\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2557\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2559\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2600\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2601\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2602\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2604\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2605\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2606\u001b[39m             {\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2621\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2622\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2623\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2625\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2626\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2627\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2628\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2629\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2630\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2631\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2632\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2633\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2634\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2635\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2636\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2637\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2638\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2639\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2640\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2641\u001b[39m             },\n\u001b[32m   2642\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2643\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2644\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2645\u001b[39m         ),\n\u001b[32m   2646\u001b[39m         options=make_request_options(\n\u001b[32m   2647\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2648\u001b[39m         ),\n\u001b[32m   2649\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2650\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2651\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2652\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\openai\\_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Work\\camp-hackathon\\udemy\\auto-gen\\udemy-auto-gen-venv\\Lib\\site-packages\\openai\\_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'User not found.', 'code': 401}}"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load API Key\n",
    "load_dotenv()\n",
    "\n",
    "open_router_model_client = OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"google/gemini-2.0-flash-exp:free\",\n",
    "    api_key=\"\",\n",
    "    model_info={\n",
    "        \"family\":'google',\n",
    "        \"vision\":False,\n",
    "        \"function_calling\":True,\n",
    "        \"json_output\":False,\n",
    "        \"structured_output\":True\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "assistant_agent1 = AssistantAgent(\n",
    "    name=\"helpful_agent\",\n",
    "    model_client=open_router_model_client,\n",
    "    system_message='You are a helpfull assistant agent'\n",
    ")\n",
    "\n",
    "result = await assistant_agent1.run(task='hey how are you today?')\n",
    "print(result.messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (udemy-auto-gen)",
   "language": "python",
   "name": "udemy-auto-gen-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
